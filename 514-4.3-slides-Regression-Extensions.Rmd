---
title: "Linear regression in R"
subtitle: |
  | Topic 4.4. Prediction intervals; 
  | The 'why' & 'who cares' of regression 
author: |
  | Sonja PetroviÄ‡
  | Created for ITMD/ITMS/STAT 514
date: "Spring 2021." 
urlcolor: darkblue
output: 
  beamer_presentation:
    slide_level: 2
    theme: "CambridgeUS"
    colortheme: "beaver"
    includes:
      in_header: header_beamer.tex
---


## Goals for this lecture 

* Understand & interpret intervals (confidence & prediction) in multiple and simple linear regression
* Fit a regression model in `Python`
* Understand & interpret `Python` output for linear models
* Many other considerations for regression modeling
   * qualitative predictors [another semester]
   * extensions of the linear model [removing additive assumption; non-linear relationships]
   * potential problems [simple overview].

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,comment = '', fig.align = 'center', out.width="70%"
)
knitr::include_graphics
```

<!--
## Prerequisites to run the code for this lecture? 

### Loading the packages we'll need: 
```
library(tidyverse)
library(GGally)
library(knitr)
```
```{r comment=FALSE, include=FALSE}
library(tidyverse)
library(GGally)
library(knitr)
```

--> 

# Prediction using regression  

## The starting point of regression

What are we *really* modeling here?

* Previously: 
  * estimating parameters based on an iid sample $Y_1,\dots,Y_n$ 
  * $\implies E[Y_1]=\dots=E[Y_n]$. 
  * in particular, $E[Y]$ does not depend on athe value of any other variable.
* Regression: 
\pause 
  * random variable $Y$ has a mean that depends on (one or several) non-random vars $X_1,\dots,X_p$ (predictors)
  * Deterministic model: $Y=\beta_0+\beta_1X$
  * **Probabilistic model**: $E[Y]= \beta_0+\beta_1X$. Equivalently: \pause 
$$Y=\underbrace{\beta_0+\beta_1X}_{deterministic\quad output} +\underbrace{\epsilon}_{random\quad  output }$$

  * Least squares estimates 
$\hat\beta_0$ and $\hat\beta_1$ minimize the RSS. 

$\rightarrow$ so... now what?? 
  
## Recall this example from previous lecture

```{r include=FALSE}
require(ISLR)
data(Auto)
fit.lm <- lm(mpg ~ horsepower, data=Auto)
```

* `Auto` data set, regression on `Y=mpg` vs. `X=horsepower`. 
```
fit.lm <- lm(mpg ~ horsepower, data=Auto)
```

* What is the predicted `mpg` associated with a `horsepower` of $98$? What are the associated 95% confidence and prediction intervals?

\tiny   
```{r}
new <- data.frame(horsepower = 98)
predict(fit.lm, new) # predicted mpg
predict(fit.lm, new, interval="confidence") # conf interval
predict(fit.lm, new, interval="prediction") # pred interval
```

   * $\rightarrow$ confidence interval vs. prediction interval $\leftarrow$ 

## Confidence vs. prediction intervals

Three sorts of uncertainty associated with the prediction of $Y$ based on $X_1,\dots,X_p$: 

* $\hat\beta_i\approx\beta_i$: least squares plane is **an estimate** for the true regression plane. 
  * reducible error
* assuming a linear model for $f(X)$ is usually an approximation of reality
  * model bias [potential reducible error?]
  * to operate here, we ignore this discrepancy 
* even if we knew true $\beta_i$, still no perfect knowledge of $Y$ because of random error $\epsilon$
  * irreducible error 
  - how much will $Y$ vary from $\hat Y$? 
  - we use *prediction intervals*. Always wider than confidence intervals.
  
## Example: `Advertising` confidence 

\redblock{{Confidence interval} Quantify the uncertainty surrounding
the average sales over a large number of cities.} 
For example:

* given that
$100,000 is spent on TV advertising and 
* $20,000 is spent on radio advertising
in each city, 
* the 95 % confidence interval is [10,985, 11,528]. 
* We interpret
this to mean that 95 % of intervals of this form will contain the true value of $f(X)$. 

## Example: `Advertising` prediction 

\redblock{{Prediction interval} Can be used to quantify the uncertainty surrounding sales for a particular city. }

* Given that $100,000 is spent on TV advertising and 
* $20,000 is spent on radio advertising in that city 
* the 95 % prediction interval is [7,930, 14,580].
* We interpret this to mean that 95 % of intervals of this form will contain the true value of Y for this city.

$\rightarrow$ Note that both intervals are centered at 11,256, but that the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about sales for a given city in comparison to the average sales over many locations.




# Other considerations and extensions 

## Qualitative predictors

\say{This is out of scope this semester (we are out of time!), but consider this setup:}

* there may be a *qualitative predictor* (that, is a discrete random variable $X_i$) -- it's also called a **factor** 
* suppose $X_i$ has only two levels (e.g. female and not female)
* we use a *dummy variable* 

\[ x_i = \begin{cases} 1 \mbox{, if $i$th person is female} \\ 0\mbox{, if $i$th person is not female} \end{cases}
\]

* use this as predictor in the regression equation.

### The model becomes:

\[ y_i=\beta_0+\beta_1x_i +\epsilon = 
\begin{cases} 
\beta_0+\beta_1+\epsilon_i \mbox{, if $i$th person is female} \\ 
\beta_0+\epsilon_i \mbox{, if $i$th person is not female} 
\end{cases}
\]

--- 

### The model becomes:

\[ y_i=\beta_0+\beta_1x_i +\epsilon = 
\begin{cases} 
\beta_0+\beta_1+\epsilon_i \mbox{, if $i$th person is female} \\ 
\beta_0+\epsilon_i \mbox{, if $i$th person is not female} 
\end{cases}
\]

### Interpret: 
  * $\beta_0$ = average $Y$ among non-females 
  * $\beta_0+\beta_1$ = average $Y$ among females
  * $\beta_1$ average difference in $Y$ between the two groups.


## Extensions of the linear model

> What is wrong with the linear model? It works quite well! 

Yes -- but sometimes the (restrictive) assumptions are violated in practice.

### Assumption 1: additivity

The relationship between the predictors and response is additive. 

* effect of changes in a predictor $X_j$ on the response $Y$ is independent of the values of the other predictors. 

### Assumption 2: linearity

The relationship between the predictors and response is linear. 

* the change in the response $Y$ due to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$. 

## Removing the additive assumption 

Previous analysis of `Advertising` data: 
both `TV` and `radio` seem associated with sales. 

* The linear models that formed the basis for this conclusion: 
\[sales=\beta_0+ \beta_1 TV + \beta_2 radio + \beta_3 newspaper +\epsilon\]

* We will now [in the notes] explain how to augment this model by allowing \attn{interaction between} `radio` and `TV` in predicting `sales`: 
\pause 

\[ 
sales =\beta_0+\beta_1\times TV+\beta_2\times radio +\beta_3 \times (radio\times TV)+\epsilon 
\]
\[\tag{\textcolor{blue}{3.33}}
\quad = \beta_0+(\beta_1+\beta_3\times radio)\times TV + \beta_2\times radio + \epsilon. 
\]

### Interpretation: 

  *   $\beta_3$ =  increase in the effectiveness of TV advertising for a one unit increase in radio advertising (or vice-versa).
  
--- 

![ISLR Table 3.9](img/ISLRtable3.9.pdf)

### Discuss: 

* main effects
* hierarchical principle 

## Removing the linear assumption 

\small 
\redblock{{Polynomial regression}models non-linear relationships.}


![\tiny ISLR Fig. 3.8. The `Auto` data set. For a number of cars, `mpg` and `horsepower` are shown. The linear regression fit is shown in orange. The linear regression fit for a model that includes `horsepower^2` is shown as a blue curve. The linear regression fit for a model that includes all polynomials of `horsepower` up to fifth-degree is shown in green.](img/ISLRfig3.8.pdf){width=70%}

--- 

![ISLR Fig. 3.8.](img/ISLRfig3.8.pdf){width=50%}

![ISLR Table 3.10](img/ISLRtable3.10.pdf)

# Potential problems

## Common issues and problems


1. Non-linearity of the response-predictor relationships. 2. Correlation of error terms.
3. Non-constant variance of error terms.
4. Outliers.
5. High-leverage points.
6. Collinearity

\say{(You will learn to deal with these in another course that focuses more on using regression in your application domain.)
}


##  License  

This document  is created for ITMD/ITMS/STAT 514, Spring 2021, at Illinois Tech. 
While the course materials are generally not to be distributed outside the course without permission of the instructor, all materials posted on this page are licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).


Contents of this lecture is based on the chapter 3 of the textbook Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 
*' An Introduction to Statistical Learning: with Applications in R'*. 
