---
title: "Introduction to Statistical Learning"
subtitle: |
  | -- (Part Two) --
  | Topic 3. Statistical learning - a high-level overview and illustrative examples 
  | 3.2. Estimation: how and why; tradeoff between accuracy and interpretability
author: |
  | Sonja Petrović
  | Created for ITMD/ITMS/STAT 514
date: "Spring 2021." 
urlcolor: darkblue
output: 
  beamer_presentation:
    slide_level: 2
    theme: "CambridgeUS"
    colortheme: "beaver"
    includes:
      in_header: header_beamer.tex
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,comment = '', fig.align = 'center', out.width="40%"
)
knitr::include_graphics
```


## Goals of this lecture

* Setting the  context: estimating $f$
* Accuracy-interpretability trade off 
    * [looking forward to the bias-variance trade off]
* Supervised vs. unsupervised learning
* Regression vs. classification


# Setting the context: estimating $f$, accuracy & interpretability

## Review: estimating $f$ [regression setting]

* Observe: a quantitative response $Y$, $p$ different predictors, $X_1,X_2,\dots,X_p$. 
* Assume: some relationship between $Y$ and $X = (X_1,X_2,...,X_p)$, which can be written in the very general form
\[ Y =f(X)+\epsilon.\]

* $f$ is some \attn{fixed but unknown} function of $X_1,X_2,\dots,X_p$
![](img/ISLRfig2.3.pdf){width=20%}

* $\epsilon$  is a random error term, which is independent of $X$ and has mean zero. 

* In this formulation, \attn{$f$ represents the *systematic* information that $X$ provides about $Y$}. 


##   $f$,   $Y=f(X)+\epsilon$,  $\hat{f}$,  $\hat{Y}=\hat{f}(X)$ \qquad [regression setting]


![ISLR figure](img/ISLRfig2.3.pdf){ width=45% }
![ISLR figure](img/ISLRfig2.4.pdf){ width=45% }

![ISLR figure](img/ISLRfig2.5.pdf){ width=45% }
![ISLR figure](img/ISLRfig2.6.pdf){ width=45% }




## Accuracy vs. interpretability


* **Less flexible** methods = more restrictive, relatively small range of shapes for $\hat{f}$. 
  * E.g.: linear regression
  

* **More flexible** methods = can generate a wider range of possible shapes to estimate $f$.
\pause 

* **Why** ever choose more restrictive?!   
  * \attn{Inference}: restrictive $\leftrightarrow$ interpretable
    * E.g. Linear model: easy to  understand the relationship between $Y$  anad $X_1,\dots,X_p$. 
    * Flexible approach can lead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response. 
  * \attn{Prediction}: the interpretability of the predictive model is simply not of interest
    * Expect? -  best to use most flexible model 
    * Surprise: often more accurate prediction  using a  less flexible method *(looking ahead: the overfitting phenomenon)*.
    
## Generalizability: a central theme

 Construct predictors that generalize well to unseen data
 
  * Capture \attn{useful trends} in the data *(don’t underfit)*
  * Ignore \attn{meaningless random fluctuations} in the data *(don’t overfit)*

![meaning of overfitting and underfitting](img/UnderVsOverfitting.pdf)  

--- 

 Avoid unjustifiably extrapolating beyond the scope of the data

![meaningless extrapolation](img/meaninglessExtrapolation.pdf) 



## Reminder: supervised vs unsupervised learning


* \attn{Predictive} Analytics (Supervised learning):
  * Q: To whom should I extend credit?   
    * \attn{Task:} Predict how likely an applicant is to repay loan.
  * Q: What characterizes customers who are likely to churn?  
    * \attn{Task:} Identify variables that are predictive of churn.
  * Q: How profitable will this subscription customer be?   
    * \attn{Task:} Predict how long customer will remain subscribed.



* \attn{Descriptive} Analytics (Unsupervised learning):
  * \attn{Clustering} customers into groups with similar spending habits
  * Learning \attn{association rules}: E.g., 50% of clients who {recently got
promoted, had a baby} want to {get a mortgage}

## Supervised vs. unsupervised  -- from $f$'s point of view:

\redblock{{Supervised learning}
 For each observation of the predictor measurement(s) $x_1,\dots,x_n$,
 
 there is an associated response measurement $y_i$.  
}

\redblock{{Unsupervised learning}
for every observation $i=1,\dots,n$,  we observe a vector of measurements $x_i$ but no associated response $y_i$. 
}

## Regression vs. classification

### Types of random variables: 
quantitative (continuous) or qualitative (categorical, discrete). 

### 
We select learning methods based on type of response (predictor type less important)! 

* Quantitative response $\mapsto$ regression problems
* Qualitative response $\mapsto$ classification problems
  * ... *but the lines do blur, so beware:* 
    * Least squares linear regression is used with a quantitative response, 
    * Logistic regression is typically used with a qualitative (two-class, or binary) response. As such it is often used as a classification method. 

  

## $\rightarrow$ Up next: $\leftarrow$ 


  * Assessing model accuracy 
      * (from the point of view of both classification and regression)
        - [NEXT LECTURE]
  * Training & testing data sets  
      * Partitioning 
      * Balancing
      * Cross-validation, etc. 
        - [NEXT LECTURE; 
        but in preparation for that:  HANDS-ON LAB NOW]

\redblock{{Aha!}
It is time for AhaSlides review!
https://www.ahaslides.com/STATITMW11
}      


##  Lab time!

$\rightarrow$ Hands-on: group breakout work $\leftarrow$ 

See worksheets handouts posted on Campuswire:

* Partitioning the data
* Validating the partition
* Balancing




##  License  

This document  is created for ITMD/ITMS/STAT 514, Spring 2021, at Illinois Tech. 
While the course materials are generally not to be distributed outside the course without permission of the instructor, all materials posted on this page are licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).

Content of this lecture is based on the first two chapters of the textbook Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, *' An Introduction to Statistical Learning: with Applications in R'*. 
The book is available online. 

Part of this lecture  notes are extracted from Prof. Alexandra Chouldechova data mining notes CMU-95791, 
released under a Attribution-NonCommercial-ShareAlike 3.0 United States license.
